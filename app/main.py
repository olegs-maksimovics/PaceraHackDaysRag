import streamlit as st
import time

st.set_page_config(page_title="RAG Demo", page_icon="ðŸ¤–")

st.title("ðŸ¤– RAG Question Answering")
st.markdown("""
This is a dummy RAG interface. Ask a question about your documents, and the system will (mock) retrieve relevant sections and generate an answer.
""")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "citations" in message:
            with st.expander("Sources"):
                for source in message["citations"]:
                    st.write(f"- {source}")

if prompt := st.chat_input("Ask a question about your documents..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # Mock RAG response
        dummy_answer = f"Based on the documents provided, here is a mock answer to your question: '{prompt}'. In a real RAG system, this would be generated by an LLM using retrieved document chunks."
        dummy_citations = [
            "Source: rag-project-overview.md (Section: Success Criteria)",
            "Source: RAG_Mastery_Building_Dynamic_AI.pdf (Page 12)"
        ]

        # Simulate streaming
        for chunk in dummy_answer.split():
            full_response += chunk + " "
            time.sleep(0.05)
            message_placeholder.markdown(full_response + "â–Œ")

        message_placeholder.markdown(full_response)

        with st.expander("Sources"):
            for source in dummy_citations:
                st.write(f"- {source}")

        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response,
            "citations": dummy_citations
        })
